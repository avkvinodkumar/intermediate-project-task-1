Here is an example of what the source code for the GitHub repository might look like:
```
data_preprocessing.ipynb
=====================

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('customer_churn_data.csv')

# Preprocess the data
df = df.dropna()  # drop rows with missing values
df['age'] = df['age'].apply(lambda x: x / 100)  # scale age by dividing by 100
df['tenure'] = df['tenure'].apply(lambda x: x / 100)  # scale tenure by dividing by 100

# One-hot encode categorical variables
df = pd.get_dummies(df, columns=['plan_type'])

# Scale numerical variables using StandardScaler
scaler = StandardScaler()
df[['age', 'tenure', 'data_usage', 'voice_minutes', 'text_messages']] = scaler.fit_transform(df[['age', 'tenure', 'data_usage', 'voice_minutes', 'text_messages']])

# Save the preprocessed data to a new CSV file
df.to_csv('preprocessed_data.csv', index=False)

feature_selection.ipynb
=====================

# Import necessary libraries
import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif

# Load the preprocessed data
df = pd.read_csv('preprocessed_data.csv')

# Select the top 5 features using f_classif
selector = SelectKBest(f_classif, k=5)
selector.fit(df.drop('churned', axis=1), df['churned'])
selected_features = selector.transform(df.drop('churned', axis=1))

# Print the selected features
print(selected_features.shape)

model_training.ipynb
=====================

# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load the preprocessed data
df = pd.read_csv('preprocessed_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(selected_features, df['churned'], test_size=0.2, random_state=42)

# Train a random forest classifier on the training set
rfc = RandomForestClassifier(n_estimators=100, random_state=42)
rfc.fit(X_train, y_train)

# Evaluate the model on the testing set
y_pred = rfc.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1-score:", f1_score(y_test, y_pred))

model_evaluation.ipynb
=====================

# Import necessary libraries
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the preprocessed data
df = pd.read_csv('preprocessed_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(selected_features, df['churned'], test_size=0.2, random_state=42)

# Evaluate the model on the testing set
y_pred = rfc.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1-score:", f1_score(y_test, y_pred))

model_deployment.py
====================

# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Load the preprocessed data
df = pd.read_csv('preprocessed_data.csv')

# Load the trained model from disk
rfc = pickle.load(open('rfc_model.pkl', 'rb'))

# Make predictions on new data using the trained model
new_data = pd.DataFrame({'age': [30], 'tenure': [2], 'plan_type': ['gold'], 'data_usage': [100], 'voice_minutes': [50], 'text_messages': [20]})
new_data = pd.get_dummies(new_data, columns=['plan_type'])
new_data[['age', 'tenure', 'data_usage', 'voice_minutes', 'text_messages']] = StandardScaler().fit_transform(new_data[['age', 'tenure', 'data_usage', 'voice_minutes', 'text_messages']])
new_data = pd.DataFrame(rfc.predict(new_data.drop('churned', axis=1)))

print(new_data)

requirements.txt
================

pandas==1.2.4
numpy==1.21.2
scikit-learn==0.24.2
